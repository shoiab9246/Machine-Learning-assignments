{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW03 : Optimization theory and gradient descent (for Uni- and bi-variate functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Implement  a solver for  minimization problems by yourself. \n",
    "\n",
    "The function find_root should have following parameters:<br>\n",
    "Parameters: <br>\n",
    "        df (func) : first derivative function<br>\n",
    "        eta (float) : learning rate<br>\n",
    "        max_iter (int) : maximum number of iterations<br>\n",
    "        tol (float) : tolerance<br>\n",
    "        root (float) : initial guess for root of the gradient function<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The function should return :<br>    \n",
    "    Returns:<br>\n",
    "        root (float) : root of function<br>\n",
    "        num_iters (int) : the number of iterations<br>\n",
    "\n",
    "**Please document your code with docstrings and comments. Please make sure your response here uses the preformatted style. **\n",
    "\n",
    "```\n",
    "# Notice how the code is in preformatted style\n",
    "the def test_func():\n",
    "\"\"\" This is a doc string\"\"\"\n",
    "   pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root(df, eta, max_iter, tol, root):\n",
    "    \"\"\"\n",
    "    This function will return the root and number of gradient descent iterations to solve f(x)\n",
    "    \n",
    "    Parameters:\n",
    "        df (func) : first derivative function\n",
    "        eta (float) : learning rate\n",
    "        max_iter (int) : maximum number of iterations\n",
    "        tol (float) : tolerance\n",
    "        root (float) : initial guess for root of the gradient function\n",
    "        \n",
    "    Returns:\n",
    "        root (float) : root of function\n",
    "        i (int) : the number of iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize delta\n",
    "    delta = float('inf')\n",
    "    \n",
    "    i = 0\n",
    "    while delta > tol and i < max_iter:\n",
    "        #==================================================#\n",
    "        #               Place your code between here       #\n",
    "        x = root\n",
    "        root = ...\n",
    "        delta = abs(...)\n",
    "        i+=1\n",
    "        #==================================================#\n",
    "        \n",
    "    return root, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here is the answer for Q 1. Please go through the code to understand the find_root function.\n",
    "def find_root(df, eta, max_iter, tol, root):\n",
    "    \"\"\"\n",
    "    This function will return the root and number of gradient descent iterations to solve f(x)\n",
    "    \n",
    "    Parameters:\n",
    "        df (func) : first derivative function\n",
    "        eta (float) : learning rate\n",
    "        max_iter (int) : maximum number of iterations\n",
    "        tol (float) : tolerance\n",
    "        root (float) : initial guess for root of the gradient function\n",
    "        \n",
    "    Returns:\n",
    "        root (float) : root of function\n",
    "        num_iters (int) : the number of iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    delta = float('inf')\n",
    "    \n",
    "    i = 0\n",
    "    while delta > tol and i < max_iter:\n",
    "\n",
    "        new_root = root\n",
    "        root = new_root - eta * df(new_root)\n",
    "        delta = abs(new_root - root)\n",
    "        i+=1\n",
    "        \n",
    "    return root, i\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Using the code you wrote in Question 7, if $f(x) = x^2$, given $x$ in the domain of [-2, 2], find argmin $f(x)$ . Sometimes this is written as find x such that it  minimizes $f(x)$. Here use gradient descent along with a learning rate of   0.1 and a starting guess for the root of the gradient function of 1.5.   Report the minimum that you attain  via your homegrown gradient descent \n",
    "<select name=\"question 7.1\" id=\"q7.1\">\n",
    "    <option value=\"[Select]\" selected>[Select]</option>\n",
    "    <option value=\"-0.000005\">-0.000005</option>\n",
    "    <option value=\"0.0\">0.0</option>\n",
    "    <option value=\"0.000005\">0.000005</option>\n",
    "    <option value=\"0.00003\" >0.00003</option>\n",
    "</select>\n",
    " when run for  100 iterations (if required) when the tolerance for convergence is 1e-5.  How many gradient descent iterations did you take? \n",
    "<select name=\"question 7.2\" id=\"q7.2\">\n",
    "    <option value=\"[Select]\" selected>[Select]</option>\n",
    "    <option value=\"1\">1</option>\n",
    "    <option value=\"9\">9</option>\n",
    "    <option value=\"100\">100</option>\n",
    "    <option value=\"48\" >48</option>\n",
    "</select>\n",
    " . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative function\n",
    "def df(x):\n",
    "    #==================================================#\n",
    "    #               Place your code between here       #\n",
    "    return ...\n",
    "    #==================================================#\n",
    "\n",
    "root, num_iters = find_root(df, eta=0.1, max_iter=100, tol=1e-5, root=1.5)\n",
    "print(format(np.round(root,5), '.05f'), num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the answer for Question 2\n",
    "def df(x):\n",
    "    #==================================================#\n",
    "    #               Place your code between here       #\n",
    "    return 2 * x\n",
    "    #==================================================#\n",
    "\n",
    "root, num_iters = find_root(df, eta=0.1, max_iter=100, tol=1e-5, root=1.5)\n",
    "print(format(np.round(root,5), '.05f'), num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Similarily using the code from lab where convergence is defined as the difference of the derivative of the root to zero, if $f(x) = x^2$, given $x$ in the domain of [-2, 2], find argmin $f(x)$ . Sometimes this is written as find x such that it  minimizes $f(x)$. Here use gradient descent along with a learning rate of   0.1 and a starting guess for the root of the gradient function of 1.5.   Report the minimum that you attain  via your homegrown gradient descent \n",
    "<select name=\"question 7.1\" id=\"q7.1\">\n",
    "    <option value=\"[Select]\" selected>[Select]</option>\n",
    "    <option value=\"-0.000005\">-0.000005</option>\n",
    "    <option value=\"0.0\">0.0</option>\n",
    "    <option value=\"0.000005\">0.000005</option>\n",
    "    <option value=\"0.00003\" >0.000004</option>\n",
    "</select>\n",
    " when run for  100 iterations (if required) when the tolerance for convergence is 1e-5.  How many gradient descent iterations did you take? \n",
    "<select name=\"question 7.2\" id=\"q7.2\">\n",
    "    <option value=\"[Select]\" selected>[Select]</option>\n",
    "    <option value=\"1\">1</option>\n",
    "    <option value=\"9\">9</option>\n",
    "    <option value=\"100\">100</option>\n",
    "    <option value=\"48\" >57</option>\n",
    "</select>\n",
    " . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Terminating criteria is df(x) == 0, i.e., is x a root of f(x)\n",
    "def terminating_criteria(df, x):\n",
    "    \"\"\"examine x for progress; \n",
    "     if root estimate is good then df(x) should be close to ZERO\n",
    "     \"\"\"\n",
    "    return abs(0-df(x))\n",
    "\n",
    "def gradient_descent(df, d2f, x0, tol=0.001, alpha = .5, \n",
    "                    print_res=False, track_every_n_steps=10, num_of_iters=1000):\n",
    "    \"\"\"Find the root of the provide gradient function via \n",
    "       gradient descent algorithm\n",
    "    \n",
    "    Args:\n",
    "        df  (func): The gradient function for which we want its roots x i.e., f(x) ==0.\n",
    "        d2f (func): not used in gradient descent: why? (hint: we use alpha the learning rate instead)\n",
    "        x0  (double): initial root guess for the gradient function df\n",
    "        tol (double): tolerance for when we stop searching; we would like df(x) == 0 within a tolerance\n",
    "        eta (double): the learning rate or step size; sometimes called alpha\n",
    "        print (bool): print a trace or not of the different root estimates that we explored\n",
    "        num_of_iters: how many iterations should the gradient descent algorithm run for\n",
    "\n",
    "    Returns:\n",
    "        (x0, approximations): root and the root approximations\n",
    "\n",
    "    \"\"\"\n",
    "    root_estimates = [x0]  #track the root estimates\n",
    "    gradients=[]  #track the gradients at each iteration of GD\n",
    "    i=0\n",
    "    while terminating_criteria(df, x0) > tol and i < num_of_iters:\n",
    "        #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "        if (i%track_every_n_steps==0): gradients.append(df(x0)) #track every n gradients\n",
    "        #print(f\"x0 = x0 - alpha * df(x0) {(x0 - alpha * df(x0)):7.3f} = {x0:7.3f} - {alpha:7.5f} * {df(x0):7.3f}\")\n",
    "        x0 = x0 - alpha * df(x0)   # Gradient step\n",
    "        if (i%track_every_n_steps==0): root_estimates.append(x0); gradients.append(df(x0)) #track every n estimates\n",
    "        i+=1\n",
    "    if print_res: print ('Root is at: ', x0); print ('df(x) at root is: ', df(x0))\n",
    "    return (x0, root_estimates, gradients, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    #==================================================#\n",
    "    #               Place your code between here       #\n",
    "    return ...\n",
    "    #==================================================#\n",
    "\n",
    "def d2f(x):\n",
    "    #==================================================#\n",
    "    #               Place your code between here       #\n",
    "    return ...\n",
    "    #==================================================#\n",
    "\n",
    "root_homegrown, trace, gradients, iter_cnt = gradient_descent(df, d2f, 1.5,\n",
    "                                                                tol=1e-5, alpha=0.1, track_every_n_steps=1)\n",
    "\n",
    "print(format(np.round(root_homegrown, 6), '.06f'), iter_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "If $f(x) = 5x^2 +7x$, given x in the domain of [-3, 3], find argmin $f(x)$ . Sometimes this is written as find x such that it  minimizes $f(x)$. Here use a learning rate of 0.1 and use a starting guess for the root of gradient function of  10.77.   Report the minimum that you attain  via your homegrown gradient descent \n",
    "<select name=\"question 9.1\" id=\"q9.1\">\n",
    "    <option value=\"[Select]\" selected>[Select]</option>\n",
    "    <option value=\"-0.7\" >-0.7</option>\n",
    "    <option value=\"0.7\">0.7</option>\n",
    "    <option value=\"-1.4\">-1.4</option>\n",
    "    <option value=\"1.4\">1.4</option>\n",
    "</select>\n",
    " when run for  100 iterations (if required) when the tolerance for convergence is 1e-5.  How many gradient descent iterations did you take? \n",
    "<select name=\"question 9.1\" id=\"q9.1\">\n",
    "    <option value=\"[Select]\" selected>[Select]</option>\n",
    "    <option value=\"2\" >2</option>\n",
    "    <option value=\"28\">28</option>\n",
    "    <option value=\"100\">100</option>\n",
    "    <option value=\"Correct Solution not provided\">Correct Solution not provided</option>\n",
    "</select>\n",
    " . \n",
    " \n",
    "**Please use your own homegrown function you created in question 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    #==================================================#\n",
    "    #               Place your code between here       #\n",
    "    return ...\n",
    "    #==================================================#\n",
    "    \n",
    "root, num_iters = find_root(df, eta=0.1, max_iter=100, tol=1e-5, root=10.77)\n",
    "print(root, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "If $f(x) = 2x^2 +4x$, given x{-3, 3}, find argmin $f(x)$ . Sometimes this is written as find x such that it  minimizes $f(x)$. Here use gradient descent along with a learning rate of 0.5, and a starting guess for the root of gradient function of  2.7.   Report the minimum that you attain  via your homegrown gradient descent \n",
    "<select name=\"question 10.1\" id=\"q10.1\">\n",
    "    <option value=\"[Select]\" selected>[Select]</option>\n",
    "    <option value=\"1\">-1</option>\n",
    "    <option value=\"28\">-0.999998</option>\n",
    "    <option value=\"100\">2.7</option>\n",
    "    <option value=\"None of these answers are correct\">None of these answers are correct.</option>\n",
    "</select>\n",
    " when run for  10 iterations (if required) when the tolerance for convergence is 1e-5.  Gradient  descent will converge  on a good root after how many iterations?  \n",
    "<select name=\"question 10.2\" id=\"q10.2\">\n",
    "    <option value=\"[Select]\" selected>[Select]</option>\n",
    "    <option value=\"1\">1 iteration of gradient descent</option>\n",
    "    <option value=\"28\">100 or so iterations of gradient descent</option>\n",
    "    <option value=\"1000\">Gradient descent will never converge under this setting. If the learning is changed to a smaller value, it may converge.</option>\n",
    "    <option value=\"100 or so iterations of gradient descent\">1000 or so iterations of gradient descent</option>\n",
    "</select>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    #==================================================#\n",
    "    #               Place your code between here       #\n",
    "    return ...\n",
    "    #==================================================#\n",
    "\n",
    "root, num_iters = find_root(df, eta=0.5, max_iter=10, tol=1e-5, root=2.7)\n",
    "print(root, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "If $f(x) = 4x^3 + 9x^2 + 2x-1$ (a univariate function), given x in the domain of [-3, 3], find argmin f(x) . Sometimes this is written as find x such that it  minimizes f(x). Here use gradient descent along with a learning rate of  0.01, and use a starting guess for the root of gradient function of  1.5.   Report the minimum up to three decimal places that you attain  via your homegrown gradient descent  when run for  100 iterations (if required) when the tolerance for convergence is 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    #==================================================#\n",
    "    #               Place your code between here       #\n",
    "    return ...\n",
    "    #==================================================#\n",
    "\n",
    "root, num_iters = find_root(df, eta=0.01, max_iter=100, tol=1e-5, root=1.5)\n",
    "np.round(root,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Implement  a solver for  multivariate minimization problems by yourself. And show it in action for  the following:\n",
    "\n",
    "If $f(x) = (x_2 - x_1)^4 + 8x_1x_2 - x_1 + x_2 + 3$, given Xs in the domain of [-3, 3], find argmin $f(X)$ . Sometimes this is written as find X such that it  minimizes f(X). Here use gradient descent along with a learning rate of 0.01 and a starting guess for the root of gradient function of:  initial_root_guess = [-0.9, -0.9].   Report the value of the objective function value at the optimal setting (i.e., the minimum) for this problem  up to three decimal places that you attain  via your homegrown gradient descent  when run for  1000 iterations (if required) when the tolerance for convergence is 1e-3. Please document your code with docstrings and comments.\n",
    "\n",
    "**Please make sure your response here uses the preformatted style. You will get ZERO points for this question if the code is not formatted correctly, and if it is a copy of the lab notebook code.**\n",
    "\n",
    "```\n",
    "# Notice how the code is in preformatted style\n",
    "the def test_func():\n",
    "\"\"\" This is a doc string\"\"\"\n",
    "   pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mgd(func, theta, eta, max_iter, tol):\n",
    "    \"\"\"\n",
    "    This function will return the root and number of gradient descent iterations to solve f(x)\n",
    "    \n",
    "    Parameters:\n",
    "        func (sympy.core.add) : objective function f(x)\n",
    "        theta (list) : initial guess for gradient\n",
    "        eta (float) : learning rate\n",
    "        max_iter (int) : maximum number of iterations\n",
    "        tol (float) : tolerance\n",
    "        \n",
    "    Returns:\n",
    "        root (float) : root of function\n",
    "        theta (np.array) : gradient\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "#==========================================================#\n",
    "    #               Place your code between here       #\n",
    "    dfx1 = ...  # derivative with respect to x1\n",
    "    dfx2 = ...  # derivative with respect to x2\n",
    "    # HINT: Use sympy differentiation method. \n",
    "    #       Please refer Lab notebook detailed introduction \n",
    "    #       and examples of sympy in action\n",
    "    #       or you can refer following link-\n",
    "    #       https://docs.sympy.org/latest/tutorial/calculus.html\n",
    "    #==========================================================#\n",
    "\n",
    "\n",
    "    grad = [dfx1, dfx2]  # gradient\n",
    "    \n",
    "    # initialize\n",
    "    new_theta = np.zeros(len(theta),)\n",
    "    delta = np.array([float('inf'), float('inf')])\n",
    "    i=0\n",
    "\n",
    "    while max(delta) > tol and i < max_iter:\n",
    "        for j in range(len(grad)):\n",
    "            #==================================================#\n",
    "            #               Place your code between here       #\n",
    "            new_theta[j] = ... - ... * N(grad[j].subs([(x1, theta[0]), (x2, theta[1])]))\n",
    "            #==================================================#\n",
    "        i+=1\n",
    "        delta = abs(new_theta - theta)\n",
    "        theta = new_theta.copy()\n",
    "        \n",
    "    return float(N(func.subs(([(x1, theta[0]), (x2, theta[1])])))), theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "\n",
    "If $f(x) = (x_2 - x_1)^4 + 8x_1x_2 - x_1 + x_2 + 3$, given Xs in the domain of [-3, 3], find argmin $f(X)$ . Sometimes this is written as find X such that it  minimizes $f(X)$. Here use gradient descent along with a learning rate of 0.01 and a starting guess for the root of gradient function of:  initial_root_guess = [-0.9, -0.9].   All you need to do in this question is to run the following code cell and report the value of the objective function value at the optimal setting (i.e., the minimum) for this problem  up to three decimal places that you attain  via your homegrown gradient descent  when run for  1000 iterations (if required) when the tolerance for convergence is 1e-3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'subs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cd7e3e9d3323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#==================================================#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-af88a9bcff84>\u001b[0m in \u001b[0;36mget_mgd\u001b[0;34m(func, theta, eta, max_iter, tol)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m#==================================================#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m#               Place your code between here       #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mnew_theta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;31m#==================================================#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'subs'"
     ]
    }
   ],
   "source": [
    "x1 = Symbol('x1')\n",
    "x2 = Symbol('x2')\n",
    "\n",
    "#==================================================#\n",
    "#               Place your code between here       #\n",
    "# objective function\n",
    "f = (x2 - x1)**4 + 8*x1*x2 - x1 + x2 + 3\n",
    "\n",
    "root, gradient = get_mgd(func=f, theta=[-0.9, -0.9], eta=0.01, max_iter=1000, tol=1e-3)\n",
    "#==================================================#\n",
    "\n",
    "np.round(root,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "Upload a 3D surface plot (in JPG format)  of your multivariate optimization solution (corresponding to a bivariate problem in this case), showing the trace of the different candidate roots that were explored via gradient descent. Please use the following problem for this challenge and dont forget to report your solution on the graph:\n",
    "\n",
    "If $f(x) = (x_2 - x_1)^4 + 8x_1x_2 - x_1 + x_2 + 3$, given Xs in the domain of [-3, 3],  find argmin $f(X)$ . Sometimes this is written as find X such that it  minimizes f(X). Here use gradient descent along with a learning rate of 0.1 and a starting guess for the root of gradient function of:  initial_root_guess = [-0.9, -0.9].  On the plot report the following:\n",
    "\n",
    " The value of the objective function value at the optimal setting (i.e., the minimum) for this problem  up to three decimal places that you attain  via your homegrown gradient descent  when run for  1000 iterations (if required) when the tolerance for convergence is 1e-3. \n",
    "The optimal solution attained, i.e., x1 = ??, x2 =???]\n",
    "Do forget to label axis, and good plot title\n",
    "Feel free to add other useful information as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'subs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4ddde15fc710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mget_mgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-af88a9bcff84>\u001b[0m in \u001b[0;36mget_mgd\u001b[0;34m(func, theta, eta, max_iter, tol)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m#==================================================#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m#               Place your code between here       #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mnew_theta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;31m#==================================================#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'subs'"
     ]
    }
   ],
   "source": [
    "x1 = Symbol('x1')\n",
    "x2 = Symbol('x2')\n",
    "\n",
    "# objective function\n",
    "f = (x2 - x1)**4 + 8*x1*x2 - x1 + x2 + 3\n",
    "\n",
    "get_mgd(func=f, theta=[-0.9, -0.9], eta=0.01, max_iter=1000, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "class multivariate_GD(object):\n",
    "\n",
    "    def __init__(self,func, gradient, hessian, start_point,step_size=0.8,num_iter=100,tol=0.000001):\n",
    "        '''\n",
    "        func: function to be optimized. Takes a vector argument as input and returns\n",
    "              a scalar output\n",
    "        step_size: step size in newton method update step\n",
    "        num_iter: number of iterations for gradient descent to run\n",
    "        tol: tolerance to determine convergence\n",
    "        '''\n",
    "        self.func=func\n",
    "        self.gradient = gradient\n",
    "        self.hessian = hessian\n",
    "        self.start_point=np.array(start_point)\n",
    "        self.num_iter=num_iter\n",
    "        self.step_size=step_size\n",
    "        self.tol=tol\n",
    "        self.approach=[]\n",
    "\n",
    "    def rootFinder(self):\n",
    "        '''\n",
    "        perform multivariate newton method for function with vector input\n",
    "        and scalar output\n",
    "        '''\n",
    "        x_t=self.start_point\n",
    "        self.approach.append(x_t)\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            x_tplus1 = x_t - self.step_size * np.array(gradient(x_t))\n",
    "            #print(x_tplus1)\n",
    "            # check for convergence\n",
    "            if abs(max(x_tplus1-x_t)) < self.tol:\n",
    "                break\n",
    "            x_t = x_tplus1\n",
    "            self.approach.append(x_t)\n",
    "        #else:   #report lack of  convergence  \n",
    "        #    raise SolutionNotFound, \"No convergence after %d iterations\" % (self.num_iter)\n",
    "\n",
    "        self.crit_point = x_tplus1\n",
    "        self.max_min = self.func(x_t)\n",
    "\n",
    "        return (self.crit_point, self.max_min)\n",
    "\n",
    "    def critical_point(self):\n",
    "        '''\n",
    "        print critical point found in newton_method function. newton_method function\n",
    "        must be called first.\n",
    "        '''\n",
    "        print (self.crit_point)\n",
    "        \n",
    "\n",
    "    def plot(self):\n",
    "        samples = 100\n",
    "        x0 = np.linspace(-1.0, 1.0, samples)\n",
    "        x1 = np.linspace(-1.0, 1.0, samples)\n",
    "        x0, x1 = np.meshgrid(x0, x1)\n",
    "        D = np.empty((samples**2, samples**2))\n",
    "        D[0] = np.reshape(x0, samples**2)\n",
    "        D[1] = np.reshape(x1, samples**2)\n",
    "        Z = self.func(D)\n",
    "        Z = np.reshape(Z, (samples, samples))\n",
    "\n",
    "        fig = plt.figure(figsize=(14,10))\n",
    "        # ax = fig.add_subplot(111, projection='3d')\n",
    "        ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "        fig.add_axes(ax)\n",
    "        \n",
    "        # plot the approach\n",
    "        T = np.transpose(self.approach)\n",
    "        TZ = self.func(T)\n",
    "        ax.plot(T[0], T[1], TZ, alpha = 0.5, marker='o', c='k')\n",
    "        # ax.scatter(T[0], T[1], np.zeros(T.shape[1]), marker='o', c='K')\n",
    "\n",
    "        # Plot the surface.\n",
    "        # surf = ax.plot_surface(x0, x1, Z, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "        #                        alpha = 0.5, linewidth=0, antialiased=False)\n",
    "        surf = ax.plot_surface(x0, x1, Z, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "                               alpha = 0.5, linewidth=0, antialiased=False)\n",
    "\n",
    "        # Customize the z axis.\n",
    "        ax.set_zlim(0., 15.)\n",
    "        #ax.zaxis.set_major_locator(LinearLocator(6))\n",
    "        #ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "        # rotation of the graph\n",
    "        ax.azim = 25\n",
    "        ax.elev = 10\n",
    "\n",
    "        # Add a color bar which maps values to colors.\n",
    "        fig.colorbar(surf, shrink=0.5, aspect=10)\n",
    "\n",
    "        plt.title(\"Bivariate Gradient Descent\")\n",
    "        plt.axis((-1, 1, -1, 1))\n",
    "        plt.xlabel(\"x1 Value\")\n",
    "        plt.ylabel(\"x2 Value\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Contour plot of loss in parameter space\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        cx = plt.subplot(1, 1, 1, facecolor='Silver')\n",
    "        plt.grid(b=True, which='major', color='w', linestyle='-')\n",
    "        plt.axis((-1, 1, -1, 1))\n",
    "        plt.scatter(T[0], T[1], marker = 'o', c = 'k')\n",
    "        plt.contour(x0, x1, Z, \n",
    "                    levels=np.logspace(0.01, 2.0, 20),\n",
    "                    cmap = 'rainbow', linewidths = 2.0, alpha=0.65)\n",
    "        plt.title(\"Bivariate Gradient Descent Countour\")\n",
    "        plt.xlabel(\"x1 Value\")\n",
    "        plt.ylabel(\"x2 Value\")\n",
    "        plt.show()\n",
    "        \n",
    "        return\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # run test\n",
    "    \n",
    "    #This following function \n",
    "    #     F(x) = F(x1, x2) = (x2 - x1)^4 +8x1x2 - x1 + x2 + 3\n",
    "    # has the stationary points: \n",
    "    #    x1=[-0.42 0.42]T,   x2=[-0.13 0.13]T, and  x3=[0.55 -0.55]T\n",
    "#==================================================#\n",
    "#               Complete the following code       #\n",
    "    def objectiveFunction(x):\n",
    "        x1 = x[0]; x2 = x[1]\n",
    "        return ...\n",
    "\n",
    "    def gradient(x): #the gradient vector for which we want to find the zeros\n",
    "        x1 = x[0]; x2 = x[1]\n",
    "        return ([...,\n",
    "                ...])\n",
    "    def hessian(x):\n",
    "        x1 = x[0]; x2 = x[1]\n",
    "        return ([[12*(x2 - x1)**2, -12*(x2 - x1)**2 + 8], \n",
    "                 [-12*(x2 - x1)**2+8,  -12*(x2 - x1)**2]])\n",
    "\n",
    "    #Find candidate optima for the objective function via f (aka fprime) using Newton-Raphson\n",
    "    # xzeroGuess = [-0.2, .4]\n",
    "    xzeroGuess = [-0.9, -0.9]\n",
    "    mvGD = multivariate_GD(...,..., hessian, ..., step_size=...,num_iter=...,tol= 1e-3)\n",
    "    anOptimum = mvGD.rootFinder() #find an optimum\n",
    "    print(\"a potential optimum is located at: \", anOptimum)\n",
    "#==================================================#\n",
    "    # local minima are   x1=[-0.42 0.42]T,   x2=[-0.13 0.13]T, and  x3=[0.55 -0.55]T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = mvGD.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "Upload a 3D surface plot (in JPG format)  of your multivariate optimization solution (corresponding to a bivariate problem in this case), showing the trace of the different candidate roots that were explored via gradient descent. Please use the following problem for this challenge and dont forget to report your solution on the graph:\n",
    "\n",
    "If $f(x) = (5x_2 - 9x_1)^3 + 9x_1x_2 - x_1 + x_2 + 3$, given Xs in the domain of [-3, 3],  find argmin $f(X)$ . Sometimes this is written as find X such that it  minimizes f(X). Here use gradient descent along with a learning rate of 1e-5 and a starting guess for the root of gradient function of:  initial_root_guess = [-0.9, -0.9].  On the plot report the following:\n",
    "\n",
    " The value of the objective function value at the optimal setting (i.e., the minimum) for this problem  up to three decimal places that you attain  via your homegrown gradient descent  when run for  1000 iterations (if required) when the tolerance for convergence is 1e-3. \n",
    "The optimal solution attained, i.e., x1 = ??, x2 =???]\n",
    "Do forget to label axis, and good plot title\n",
    "Feel free to add other useful information as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================#\n",
    "#               Complete the following code       #\n",
    "def objectiveFunction(x):\n",
    "    x1 = x[0]; x2 = x[1]\n",
    "    return ...\n",
    "\n",
    "def gradient(x): #the gradient vector for which we want to find the zeros\n",
    "    x1 = x[0]; x2 = x[1]\n",
    "    return ([...,\n",
    "            ...])\n",
    "\n",
    "def hessian(x):\n",
    "    pass\n",
    "\n",
    "xzeroGuess = [-0.9, -0.9]\n",
    "mvGD = multivariate_GD(...,..., hessian, ..., step_size=...,num_iter=...,tol= 1e-3)\n",
    "#==================================================#\n",
    "\n",
    "\n",
    "anOptimum = mvGD.rootFinder() #find an optimum\n",
    "\n",
    "print(\"a potential optimum is located at: \", anOptimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = mvGD.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "383.991px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
