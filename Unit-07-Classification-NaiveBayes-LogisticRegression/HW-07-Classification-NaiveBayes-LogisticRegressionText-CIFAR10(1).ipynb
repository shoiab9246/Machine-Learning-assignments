{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 07\n",
    "\n",
    "`By   : Dr. James G. Shanahan\n",
    "EMAIL: James.Shanahan AT Gmail.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Logistic Regression basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Unnormalized perpendicular distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a three-class classification problem with the following separating hyperplanes:\n",
    "\n",
    "\\begin{align}\n",
    "1^{st\\:}class\\:\\:\\:\\:\\:\\:4x_1+x_{2\\:\\:}-2=0 \\\\\n",
    "2^{nd\\:}class\\:\\:\\:\\:\\:\\:-2x_1+2x_{2\\:\\:}-11=0 \\\\\n",
    "3^{rd\\:}class\\:\\:\\:\\:\\:-3x_1-3x_{2\\:\\:}-1=0 \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Calculate the score (unnormalized perpendicular distance) for each class for the test case $\\left(x_1,x_2\\right)=\\:\\left(-1,1\\right) $\n",
    "\n",
    "Complete the code below to get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.transpose(np.array([[4., 1.], \n",
    "              [-2., 2.], \n",
    "              [-3., -3.]])).reshape(2, 3)\n",
    "b = np.array([-2.,-11., -1.])\n",
    "x = np.array([-1., 1.])\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "\n",
    "scores= .....\n",
    "print(np.round(scores,3))\n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:  Log Loss for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two points $x_1$   and $x_2$.  $x_1$  belongs to class 0 and $x_2$  belongs to class 1 ( $y_1=0 $ and $y_2=1$) \n",
    "\n",
    "A logistic regression model predicts the class of $x_1$  with a probability of 0.3 and predicts the class of $x_2$ with a probability of 0.6\n",
    "\n",
    "The log loss formula for the binary case is as follows : $-\\frac{1}{m}\\sum^m_{i=1}\\left(y_i\\cdot\\:\\log\\:\\left(p_i\\right)\\:+\\:\\left(1-y_i\\right)\\cdot\\log\\left(1-p_i\\right)\\right) $\n",
    "\n",
    "where $m$  is the number of data points , log is Natural Logarithm\n",
    "\n",
    "Calculate the log loss for the points $x_1$  and $x_2$\n",
    "\n",
    "Please report your answer to three decimal places (e.g., report .4554 as .455)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| point| Class| Probability|\n",
    "| --- | --- | --- |\n",
    "|  $x_1$ | 0 | 0.3|\n",
    "|  $x_2$ | 1 | 0.6|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def LogLossBinary(actual, predicted, eps = 1e-15): \n",
    "    predicted = np.minimum(np.maximum(predicted, eps), 1-eps) #avoid precision problems at 0, and 1\n",
    "    #==================================================#\n",
    "    #               Your code starts here              #\n",
    "    #==================================================#\n",
    "    return( )\n",
    "    #==================================================#\n",
    "    #               Your code ends here                #\n",
    "    #               Please don't add code below here   #\n",
    "    #==================================================#\n",
    "print(f'{np.round(LogLossBinary(np.array([0, 1]),np.array([0.3, 0.6])), 3)}')\n",
    "print(f'{np.round((- np.log(1-0.3) - np.log(0.6))/2, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:  Log Loss for multinomial logistic regression\n",
    "\n",
    "The log loss (aka cross entropy) formula for the multi case is as follows : $$CXE(actual, predicted) = -\\frac{1}{m}\\sum^m_{i=1}\\left(actual_i\\cdot\\:\\log\\:\\left(predicted_i\\right)\\:)\\right) $$\n",
    "\n",
    "where $m$  is the number of data points, and $log$ is Natural Logarithm\n",
    "\n",
    "Complete the code to calculate the CXE for the test cases provided. Verify you response using the sklearn.metrics.log_loss.\n",
    "\n",
    "Please report your answer to three decimal places (e.g., report .4554 as .455)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "# homemade CXE\n",
    "def cross_entropy(predictions, targets):\n",
    "    #==================================================#\n",
    "    #               Your code starts here              #\n",
    "    #==================================================#\n",
    "    m = ...\n",
    "    cxe = -1/m*np.sum(....)\n",
    "    #==================================================#\n",
    "    #               Your code ends here                #\n",
    "    #               Please don't add code below here   #\n",
    "    #==================================================#    \n",
    "    return cxe\n",
    "\n",
    "# 2 test cases with 4 possible target classes\n",
    "predictions = np.array([[0.25,0.25,0.25,0.25],\n",
    "                        [0.01,0.01,0.01,0.97]])\n",
    "targets = np.array([[1,0,0,0],\n",
    "                   [0,0,0,1]])\n",
    "\n",
    "homemadeCXE = cross_entropy(predictions, targets)\n",
    "print(np.round(log_loss(targets, predictions),3), 'homemade CXE:', np.round(homemadeCXE,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:  Gradient Descent for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you are learning a logistic  regression model  with two a training set consisting of two examples:\n",
    "\n",
    "$x_1$  = 1 belongs to class 0 ($y_1=0$ )<br />\n",
    "$x_2$= 2 belongs to class 1 ($y_2=1 $) <br />\n",
    "\n",
    "The current model weight vector is  W = [1,1], where W[0] denotes the bias term.<br />\n",
    "\n",
    "Assume a learning rate,  $\\alpha= 0.1$ <br />\n",
    "\n",
    "Assume the gradient is defined as follows:\n",
    "\n",
    " $\\frac{\\delta E}{\\delta W} = \\frac{1}{m}\\sum^m_{i=1}\\left(p\\left(x_i\\right)-y_i\\:\\right)\\cdot X\\:\\:\\:where\\:p\\left(x\\right)\\:=\\:\\frac{1}{1+\\:e^{-W^Tx}}\\:\\: $  and $ m $ is the number of data points\n",
    "\n",
    "What is the value of W after applying one iteration of gradient descent?\n",
    "\n",
    "Please report your response to three decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X=np.array([[1,1],[1,2]])\n",
    "w= np.array([1,1])\n",
    "y= np.array([0,1])\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "perpDist= \n",
    "p =1 / (1 + np.exp(-perpDist)) #sigmoid\n",
    "gradient = \n",
    "print(f'predictions: {p}')\n",
    "print(f'Gradient: {gradient}')\n",
    "print(f'w before: {w}')\n",
    "lr = 0.1\n",
    "print(f'lr * Gradient: {lr *gradient}')\n",
    "w = \n",
    "print(f'w after: {np.round(w,3)}')       \n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOWNLOAD DATA FROM [HERE](http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) AND PUT IT TO THE DATA FOLDER\n",
    "\n",
    "The [CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks. Such a situation is called **multiclass** classification in oppose to **multilabel** classification when each example may have multiple label.\n",
    "\n",
    "One can see state-of-the-art results [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T14:22:30.010553Z",
     "start_time": "2018-06-27T14:22:28.286221Z"
    }
   },
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set style for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T14:22:30.024577Z",
     "start_time": "2018-06-27T14:22:30.019040Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", font_scale=1.3)\n",
    "matplotlib.rcParams[\"legend.framealpha\"] = 1\n",
    "matplotlib.rcParams[\"legend.frameon\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unarchive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T00:10:50.811438Z",
     "start_time": "2018-06-24T00:10:50.537432Z"
    }
   },
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"data/cifar-10-python.tar.gz\", \"r:gz\")\n",
    "tar.extractall(\"data\")\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are stored as a memory dump with python $\\text{dict}$ object. It was created using **pickle** function. To read one should \"unpickle\" it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T13:59:58.774950Z",
     "start_time": "2018-06-25T13:59:58.769771Z"
    }
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo, encoding=\"latin1\")\n",
    "    fo.close()\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are also splitted in to 5 pieces for conveniece. Let's read them all and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:07.700145Z",
     "start_time": "2018-06-24T18:04:07.096937Z"
    }
   },
   "outputs": [],
   "source": [
    "for b in range(1, 6):\n",
    "    data_batch = unpickle(\"data/cifar-10-batches-py/data_batch_\" + str(b))\n",
    "    if b == 1:\n",
    "        X_train = data_batch[\"data\"]\n",
    "        y_train = np.array(data_batch[\"labels\"])\n",
    "    else:\n",
    "        X_train = np.append(X_train, data_batch[\"data\"], axis=0)\n",
    "        y_train = np.append(y_train, data_batch[\"labels\"], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test data. In this dataset train/test split is provided by authors of the dataset to be able to consistently evaluate solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:09.412708Z",
     "start_time": "2018-06-24T18:04:09.341606Z"
    }
   },
   "outputs": [],
   "source": [
    "data_batch = unpickle(\"data/cifar-10-batches-py/test_batch\")\n",
    "X_test = data_batch[\"data\"]\n",
    "y_test = np.array(data_batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read meta-information file with the names of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:09.788097Z",
     "start_time": "2018-06-24T18:04:09.779669Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = unpickle(\"data/cifar-10-batches-py/batches.meta\")[\"label_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have too many data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:11.513813Z",
     "start_time": "2018-06-24T18:04:11.506152Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Test size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take only 10% of them to train faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:12.856320Z",
     "start_time": "2018-06-24T18:04:12.853162Z"
    }
   },
   "outputs": [],
   "source": [
    "subsample_rate = 0.1\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to preserve the same quantity ratio between classes. In python such an option is called **stratification**. Let's randomly (with fixed initial seed for the sake of reproducibility) divide part of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:13.854036Z",
     "start_time": "2018-06-24T18:04:13.672594Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, _, y_train, _ = train_test_split(X_train, y_train, stratify=y_train, train_size=subsample_rate, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:14.572748Z",
     "start_time": "2018-06-24T18:04:14.548444Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test, _, y_test, _ = train_test_split(X_test, y_test, stratify=y_test, train_size=subsample_rate, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we preserved the number of objects of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:16.945533Z",
     "start_time": "2018-06-24T18:04:16.937049Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_train = np.unique(y_train, return_counts=True)\n",
    "list(zip(np.array(classes)[unique_train[0]], unique_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now each object has the following shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:18.527866Z",
     "start_time": "2018-06-24T18:04:18.521986Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3072 = 32 \\times 32 \\times 3$ where $32 \\times 32$ is the size of the image in pixels and $3$ is the number of channels (RGB)\n",
    "\n",
    "To show this array as an image let's reshape it in the needed from with the shape $(32, 32, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T20:19:18.128589Z",
     "start_time": "2018-06-25T20:19:18.123999Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_pic(x):\n",
    "    plt.imshow(x.reshape((3, 32, 32)).transpose(1, 2, 0).astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw one pic from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:19.232922Z",
     "start_time": "2018-06-24T18:04:19.227362Z"
    }
   },
   "outputs": [],
   "source": [
    "classes_idx_examples = np.zeros(10, dtype=np.int)\n",
    "for i in range(10):\n",
    "    classes_idx_examples[i] = np.where(y_train == i)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:20.090520Z",
     "start_time": "2018-06-24T18:04:19.431438Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    show_pic(X_train[classes_idx_examples[i]])\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Sklearn implementation of algorithms to have a benchmark.\n",
    "\n",
    "Also one should **always** track the results of the experiments to be able to compare different approaches. Let's create pandas $\\text{DataFrame}$ for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:20.126348Z",
     "start_time": "2018-06-24T18:04:20.121756Z"
    }
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"Model\", \"Test Accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now it is empty, but will be filled in later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:20.284876Z",
     "start_time": "2018-06-24T18:04:20.277348Z"
    }
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Multinomial Logistic Regression (see theory in lectures)\n",
    "Sklearn implementation of LogRegression implies mandatory usage of regularization (it almost always works better with it preventing overfitting). We want to explore very basic LogRegression model thus to \"disable\" regularization we need to reduce its impact to almost zero. It can be done by setting regularization constant $\\lambda$ to very small value (in sklearn we define inverse regularization constant $C = 1 / \\lambda$ thus we need to make it big)\n",
    "\n",
    "Here we use Sklearn $\\text{LogisticRegression}$ with few options:\n",
    "* $\\text{multi_class} = \\text{\"multinomial\"} -$we want to build softmax classifier (there are other ways of dealing with multiclass setting for Logistic Regression)\n",
    "* $\\text{C} = 10^6-$ for now we don't want to use regularization; $\\text{C}$ is the inverse regularization constant which is $\\text{C} = \\frac{1}{\\lambda}$; thus we should make $\\text{C}$ big to turn off regulazrization\n",
    "* $\\text{solver} = \\text{sag} -$ optimization algorithm to use; Stochastic Average Gradient. Stochastic Gradient Descent method gitters massively. This is due to the not very good approximation of gradient (only by one example). To neglect this error one can simply average gradient across last few steps; that is exectly what $\\text{sag}$ does\n",
    "* $\\text{max_iter} = 15 -$ the number of passes over the training data (aka epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:22.043037Z",
     "start_time": "2018-06-24T18:04:22.039372Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "model_lr_sklearn = LogisticRegression(multi_class=\"multinomial\", C=1e6, solver=\"sag\", max_iter=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:40.817169Z",
     "start_time": "2018-06-24T18:04:32.628253Z"
    }
   },
   "outputs": [],
   "source": [
    "model_lr_sklearn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:46.117353Z",
     "start_time": "2018-06-24T18:04:46.097031Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test = model_lr_sklearn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:46.780505Z",
     "start_time": "2018-06-24T18:04:46.776577Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping table of results up-to-date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:04:47.450418Z",
     "start_time": "2018-06-24T18:04:47.437374Z"
    }
   },
   "outputs": [],
   "source": [
    "results.loc[len(results)] = [\"LR Sklearn\", np.round(acc, 3)]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping table of results up-to-date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments begin here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Great Race\n",
    "\n",
    "## BG: Part 1: Pima Diabetes classification\n",
    "\n",
    "It is important to compare the performance of multiple different machine learning algorithms\n",
    "consistently. In this chapter you will discover how you can create a test harness to compare\n",
    "multiple different machine learning algorithms in Python with scikit-learn. You can use this\n",
    "test harness as a template on your own machine learning problems and add more and different\n",
    "algorithms to compare. After completing this lesson you will know:\n",
    "\n",
    "1. How to formulate an experiment to directly compare machine learning algorithms.\n",
    "2. A reusable template for evaluating the performance of multiple algorithms on one dataset.\n",
    "3. How to report and visualize the results when comparing algorithm performance.\n",
    "\n",
    "In the example below six different classification\n",
    "algorithms (some of which you will recognize!) are compared on a single dataset:\n",
    "\n",
    "*  Logistic Regression\n",
    "*  Linear Discriminant Analysis\n",
    "*  k-Nearest Neighbors\n",
    "*  Classification and Regression Trees\n",
    "*  Naive Bayes\n",
    "* Support Vector Machines.\n",
    "\n",
    "The dataset is the Pima Indians onset of diabetes problem. The problem has two classes and\n",
    "eight numeric input variables of varying scales. The 10-fold cross-validation procedure is used to\n",
    "evaluate each algorithm, importantly configured with the same random seed to ensure that the\n",
    "same splits to the training data are performed and that each algorithm is evaluated in precisely\n",
    "the same way. Each algorithm is given a short name, useful for summarizing results afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common method to evaluate the model is cross-validation. The idea behind it is to divide the whole set of objects into $k$ sections and then use one section as a test set and other $k-1$ as a train (repeat it with all the sections).\n",
    "\n",
    "There is a special function for this in sklearn called $\\text{KFold}$. It creates set of indices for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T14:00:41.818148Z",
     "start_time": "2018-06-25T14:00:41.813498Z"
    }
   },
   "outputs": [],
   "source": [
    "#e.g., \n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to do everything that we've done before in a loop:\n",
    "* Split\n",
    "* Train\n",
    "* Evaluate\n",
    "\n",
    "And store the average value of the accuracy. Running the code below provides a list of each \"algorithm short name\", the mean accuracy and the standard deviation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T14:00:52.781281Z",
     "start_time": "2018-06-25T14:00:52.011997Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "# load dataset\n",
    "filename = 'data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# prepare models\n",
    "np.random.seed(42)\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "#Standardize the data\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Classification Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.grid()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we produced a box and whisker plot showing the spread of the accuracy scores\n",
    "across each cross-validation fold for each algorithm.\n",
    "\n",
    "From these results, it would suggest that both logistic regression and linear discriminant\n",
    "analysis are perhaps worthy of further study on this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: CIFAR10 Great Race \n",
    "Repeat the above RACE for the CIFAR10 dataset and draw some conclusions. Complete the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T18:05:00.458370Z",
     "start_time": "2018-06-24T18:04:59.815513Z"
    }
   },
   "outputs": [],
   "source": [
    "for b in range(1, 6):\n",
    "    data_batch = unpickle(\"data/cifar-10-batches-py/data_batch_\" + str(b))\n",
    "    if b == 1:\n",
    "        X_train = data_batch[\"data\"]\n",
    "        y_train = np.array(data_batch[\"labels\"])\n",
    "    else:\n",
    "        X_train = np.append(X_train, data_batch[\"data\"], axis=0)\n",
    "        y_train = np.append(y_train, data_batch[\"labels\"], axis=0)\n",
    "        \n",
    "data_batch = unpickle(\"data/cifar-10-batches-py/test_batch\")\n",
    "X_test = data_batch[\"data\"]\n",
    "y_test = np.array(data_batch[\"labels\"])\n",
    "\n",
    "classes = unpickle(\"data/cifar-10-batches-py/batches.meta\")[\"label_names\"]\n",
    "np.random.seed(42)\n",
    "subsample_rate = 0.02\n",
    "# Downsample the the train and test sets data\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "X_train, _, y_train, _ = train_test_split(X_train, y_train, stratify=y_train, train_size=subsample_rate, random_state=42)\n",
    "X_test, _, y_test, _ = train_test_split(...)                     \n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Great Race on CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T00:34:14.628836Z",
     "start_time": "2018-06-24T18:05:13.570032Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    # set up cross validation scores \n",
    "    #==================================================#\n",
    "    #               Your code starts here              #\n",
    "    #==================================================#\n",
    "    # set kfold for 10 folds and random_state=7, also use shuffle=True\n",
    "    kfold = \n",
    "    #set cv_results with scoring=scoring variable (which is set to 'accuracy')\n",
    "    cv_results = cross_val_score()                        \n",
    "    #==================================================#\n",
    "    #               Your code ends here                #\n",
    "    #               Please don't add code below here   #\n",
    "    #==================================================#\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Classification Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.grid()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background: HyperParameter tuning on steroids\n",
    "\n",
    "Machine learning models are parameterized so that their behavior can be tuned for a given problem. Models can have many parameters and finding the best combination of parameters can be treated as a search problem. In this section you will revisit how to tune the parameters of machine learning algorithms in Python using the scikit-learn.\n",
    "\n",
    "Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid. You can perform a grid search using the `GridSearchCV` class. \n",
    "In this section we will focus on setting up a pipeline for **text classifiction**, though it can be adapted to any machine learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pima Indian Grid Search  example\n",
    "The example below evaluates different alpha values for the Ridge Regression/LASSO algorithm on the Pima Diabetes binary classifiction data. This is a one-dimensional grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T18:48:33.125229Z",
     "start_time": "2018-06-15T18:48:33.064992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grid Search for Algorithm Tuning\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "filename = 'data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "alphas = numpy.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "param_grid = dict(alpha=alphas)\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid.fit(X, Y)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample pipeline for text feature extraction and evaluation\n",
    "\n",
    "\n",
    "The dataset used in this section is the 20 newsgroups dataset which will be\n",
    "automatically downloaded and then cached and reused for the document\n",
    "classification example.\n",
    "\n",
    "<PRE>\n",
    ">>> from sklearn.datasets import fetch_20newsgroups\n",
    ">>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    ">>> from pprint import pprint\n",
    ">>> pprint(list(newsgroups_train.target_names))\n",
    "['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']\n",
    "</PRE>\n",
    "You can adjust the number of categories by giving their names to the dataset\n",
    "loader or setting them to None to get the 20 of them.\n",
    "\n",
    "Here is a sample output of a run on a quad-core machine::\n",
    "\n",
    "<PRE>  Loading 20 newsgroups dataset for categories:\n",
    "  ['alt.atheism', 'talk.religion.misc']\n",
    "  1427 documents\n",
    "  2 categories\n",
    "  \n",
    "  \n",
    "  Performing grid search...\n",
    "  pipeline: ['vect', 'tfidf', 'clf']\n",
    "  parameters:\n",
    "  {'clf__alpha': (1.0000000000000001e-05, 9.9999999999999995e-07),\n",
    "   'clf__n_iter': (10, 50, 80),\n",
    "   'clf__penalty': ('l2', 'elasticnet'),\n",
    "   'tfidf__use_idf': (True, False),\n",
    "   'vect__max_n': (1, 2),\n",
    "   'vect__max_df': (0.5, 0.75, 1.0),\n",
    "   'vect__max_features': (None, 5000, 10000, 50000)}\n",
    "  done in 1737.030s\n",
    "\n",
    "  Best score: 0.940\n",
    "  Best parameters set:\n",
    "      clf__alpha: 9.9999999999999995e-07\n",
    "      clf__n_iter: 50\n",
    "      clf__penalty: 'elasticnet'\n",
    "      tfidf__use_idf: True\n",
    "      vect__max_n: 2\n",
    "      vect__max_df: 0.75\n",
    "      vect__max_features: 50000\n",
    "</PRE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From documents to a \"document by term\" frequency matrix  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a set of  documents to a \"document by term\" frequency matrix\n",
    "\n",
    "The following corpus: \n",
    "\n",
    "`corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]`\n",
    "\n",
    "gets converted to a \"document by term\" frequency matrix\n",
    "\n",
    "`array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
    "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
    "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
    "       [0, 1, 1, 1, 0, 0, 1, 0, 1]]\n",
    "       `\n",
    "\n",
    "where the 9 columns are labeled with the following 9 words extracted:\n",
    "\n",
    "`['and', 'document', 'first', 'is', 'one',\n",
    "      'second', 'the', 'third', 'this']`\n",
    "      \n",
    "By default, words of length 2 characters or more a kept as the vocabulary.\n",
    "\n",
    "For more details see (here)[http://scikit-learn.org/stable/modules/feature_extraction.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T18:48:47.002350Z",
     "start_time": "2018-06-15T18:48:46.998949Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let’s use the following corpus (text dataset).\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T18:48:48.533424Z",
     "start_time": "2018-06-15T18:48:48.514388Z"
    }
   },
   "outputs": [],
   "source": [
    "#CountVectorizer implements both tokenization and occurrence counting in a single class:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Here we will tokenize and count the word occurrences of this minimalistic corpus of text documents:\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X                              \n",
    "#<4x9 sparse matrix of type '<... 'numpy.int64'>'\n",
    "#    with 19 stored elements in Compressed Sparse ... format>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T18:48:53.525892Z",
     "start_time": "2018-06-15T18:48:53.519825Z"
    }
   },
   "outputs": [],
   "source": [
    "# The default configuration tokenizes the string by extracting words of at least 2 \n",
    "# letters. The specific function that does this step can be requested explicitly:  \n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"This is a text document to analyze.\") == (\n",
    "    ['this', 'is', 'text', 'document', 'to', 'analyze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T18:48:54.417175Z",
     "start_time": "2018-06-15T18:48:54.411592Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names() == (\n",
    "     ['and', 'document', 'first', 'is', 'one',\n",
    "      'second', 'the', 'third', 'this'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T18:50:40.578040Z",
     "start_time": "2018-06-15T18:50:40.566231Z"
    }
   },
   "outputs": [],
   "source": [
    "X.toarray()     #recover document by term frequency matrix        \n",
    "#array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
    "#       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
    "#       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
    "#       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note below we use a generic Linear classifier class, SGDClassifier.  Here different classifiers can be engaged by specifying different loss fucntions. Here are some of the options (note we will focus on Logistic regression that is engaged thru specifying the loss as `'log'`:\n",
    "\n",
    "\n",
    "`loss : str, default: ‘hinge’\n",
    "\n",
    "The loss function to be used. Defaults to ‘hinge’, which gives a linear SVM.\n",
    "\n",
    "The possible options are ‘hinge’, ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’, or a regression loss: ‘squared_loss’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.\n",
    "\n",
    "The ‘log’ loss gives logistic regression, a probabilistic classifier. ‘modified_huber’ is another smooth loss that brings tolerance to outliers as well as probability estimates. ‘squared_hinge’ is like hinge but is quadratically penalized. ‘perceptron’ is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on 20 newsgroups dataset\n",
    "\n",
    "The 20 newsgroups dataset has 11314 training examples, 7532 test cases.\n",
    "\n",
    "\n",
    "#### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T18:50:54.915705Z",
     "start_time": "2018-06-15T18:50:54.553993Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "trainData = fetch_20newsgroups(subset='train', categories=categories)\n",
    "data = trainData\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()\n",
    "print(\"Sample document Target Class\", data.target_names[1], )\n",
    "print(\"Sample document body\", data.data[1], )\n",
    "print(len(data.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T18:53:25.566130Z",
     "start_time": "2018-06-15T18:53:25.257704Z"
    }
   },
   "outputs": [],
   "source": [
    "print(categories)\n",
    "testData = fetch_20newsgroups(subset='test', categories=categories)\n",
    "data = testData\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()\n",
    "print(\"Sample document Target Class [%s]\"% data.target_names[1], )\n",
    "print(\"Sample document body\", data.data[1], )\n",
    "print(len(data.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T18:53:40.726487Z",
     "start_time": "2018-06-15T18:53:40.720618Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of (Train, Test) data (%d, %d)\" %(len(trainData.data),  len(testData.data) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T19:11:36.463115Z",
     "start_time": "2018-06-15T19:11:01.337976Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# This code is adopted  and has been modified from \n",
    "#\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set \n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "# categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()\n",
    "\n",
    "# #############################################################################\n",
    "# Define a pipeline combining a text feature extractor \n",
    "# with a simple classifier (logistic regression)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),  #http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    #('tfidf', TfidfTransformer()), #ignore for now\n",
    "    ('clf', SGDClassifier(loss='log', max_iter=5)),  #let's use logistic regression\n",
    "])\n",
    "\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = { #listed in the form of \"step__parameter\", e.g, clf__penalty\n",
    "    #'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # jgs 'vect__max_features': (None, 500, 5000, 10000, 50000),\n",
    "    # jgs 'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams (single words) or bigrams (or sequence of words of length 2)\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l1', 'l2', 'elasticnet'),\n",
    "    #'clf__penalty': ('l1', 'l2', 'elasticnet'),\n",
    "    #'clf__loss': ('log', 'hinge'),  #hinge linear SVM\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    # n_jobs=-1 means that the computation will be dispatched on all the CPUs of the computer.\n",
    "    #\n",
    "    # By default, the GridSearchCV uses a 3-fold cross-validation. However, if it \n",
    "    #            detects that a classifier is passed, rather than a regressor, it uses a stratified 3-fold.\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(data.data, data.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "    #print(\"grid_search.cv_results_\", grid_search.cv_results_)\n",
    "    #estimator : estimator object. This is assumed to implement the scikit-learn estimator interface.  \n",
    "    #            Either estimator needs to provide a score function, or scoring must be passed.\n",
    "    #Accuracy is the default for classification; feel free to change this to precision, recall, fbeta\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrices\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.\n",
    "\n",
    "Let's start with an example confusion matrix for a binary classifier (though it can easily be extended to the case of more than two classes):"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we learn from this matrix?\n",
    "\n",
    "* There are two possible predicted classes: \"yes\" and \"no\". If we were predicting the presence of a disease, for example, \"yes\" would mean they have the disease, and \"no\" would mean they don't have the disease.\n",
    "* The classifier made a total of 165 predictions (e.g., 165 patients were being tested for the presence of that disease).\n",
    "* Out of those 165 cases, the classifier predicted \"yes\" 110 times, and \"no\" 55 times.\n",
    "* In reality, 105 patients in the sample have the disease, and 60 patients do not.\n",
    "\n",
    "Let's now define the most basic terms, which are whole numbers (not rates):\n",
    "\n",
    "* true positives (TP): These are cases in which we predicted yes (they have the disease), and they do have the disease.\n",
    "* true negatives (TN): We predicted no, and they don't have the disease.\n",
    "* false positives (FP): We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")\n",
    "* false negatives (FN): We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")\n",
    "\n",
    "I've added these terms to the confusion matrix, and also added the row and column totals:"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image.png](attachment:image.png)\n",
    "This is a list of rates that are often computed from a confusion matrix for a binary classifier:\n",
    "\n",
    "* Accuracy: Overall, how often is the classifier correct?\n",
    "  * (TP+TN)/total = (100+50)/165 = 0.91\n",
    "* Misclassification Rate: Overall, how often is it wrong?\n",
    "  * (FP+FN)/total = (10+5)/165 = 0.09\n",
    "  * equivalent to 1 minus Accuracy\n",
    "  * also known as \"Error Rate\"\n",
    "* True Positive Rate: When it's actually yes, how often does it predict yes?\n",
    "  * TP/actual yes = 100/105 = 0.95\n",
    "  * also known as \"Sensitivity\" or \"Recall\"\n",
    "* False Positive Rate: When it's actually no, how often does it predict yes?\n",
    "  * FP/actual no = 10/60 = 0.17\n",
    "  * Specificity: When it's actually no, how often does it predict no?\n",
    "* TN/actual no = 50/60 = 0.83\n",
    "  * equivalent to 1 minus False Positive Rate\n",
    "* Precision: When it predicts yes, how often is it correct?\n",
    "  * TP/predicted yes = 100/110 = 0.91\n",
    "* Prevalence: How often does the yes condition actually occur in our sample?\n",
    "  * actual yes/total = 105/165 = 0.64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example confusion matrix for the Iris Data\n",
    "\n",
    "#See labs for this unit for more details of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(C=1e6, multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "model.fit(X_train, y_train)\n",
    "preds_test = model.predict(X_test)\n",
    "preds_train = model.predict(X_train)\n",
    "class_labels = [\"setosa\", \"versicolour\", \"virginica\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "#See labs for this unit for more details of this\n",
    "cm_train = confusion_matrix(y_train, preds_train).astype(np.float32)\n",
    "cm_train /= cm_train.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "cm_test = confusion_matrix(y_test, preds_test).astype(np.float32)\n",
    "cm_test /= cm_test.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "g = sns.heatmap(cm_train, vmin=0, vmax=1, annot=True, cmap=\"Reds\")\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"True\", fontsize=14)\n",
    "g.set(xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title(\"Train\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "g = sns.heatmap(cm_test, vmin=0, vmax=1, annot=True, cmap=\"Reds\")\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"True\", fontsize=14)\n",
    "g.set(xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title(\"Test\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusions matrices for training, setosa perfectly separates from two other classes. In the test confusion matrix we see perfect classification (very unusual and suspect in the real world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task:  20 class text classifier\n",
    "Using the gridsearch pipeline presented in the previous section, please adapt it to get your best configuration using cross fold validation on all 20 classes from the 20 newsgroups dataset. Here are some hyperparameters to consider but don't limit yourself to these:\n",
    "\n",
    "* penalty\n",
    "* number of terms\n",
    "* types of ngrams\n",
    "* linear classifier\n",
    "* TDIDF\n",
    "\n",
    "\n",
    "Have fun! Please report your best score and configuration. And discuss your confusion matrix analysis for the best configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Task: Pipeline & Grid search\n",
    "\n",
    "This grid search will take some time (at least 20 minutes or more to run on a 4-core machine). Recall the following GridSearch will use all available cores: \n",
    "\n",
    "```python\n",
    "GridSearchCV(pipelinee, parameters, cv=3, n_jobs=-1, verbose=1)\n",
    "```\n",
    "\n",
    "since `n_jobs` is set to `-1` \n",
    "\n",
    "Perform grid search where the score being used to evaluate each hyperparameter combination is `precision_macro`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T14:57:21.628707Z",
     "start_time": "2018-06-27T14:23:05.964332Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "##############################################################################\n",
    "np.random.seed(42)\n",
    "\n",
    "# Use all categories for the analysis\n",
    "\n",
    "categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset:\")\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Define a pipeline combining  text feature extractors \n",
    "#countVectorizer and TfidfTransformer\n",
    "# with SGDClassifier using log for loss and max_iter=5\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "pipeline =                     \n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # #############################################################################\n",
    "    # Set up Grid search using the defined pipeline \n",
    "    # and parameters \n",
    "    # Make sure to use 3 folds for cross validation\n",
    "    # use macro average precision scores scoring='precision_macro'\n",
    "    # I.e., macro average: compute precision for each class and take avg\n",
    "    #==================================================#\n",
    "    #               Your code starts here              #\n",
    "    #==================================================#\n",
    "    scoring='precision_macro'\n",
    "    # select handful of parameters to explore\n",
    "    parameters = {'vect__ngram_range': ((1,1),(1,2)),\n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'clf__alpha': (.001, .0001), \n",
    "                 'clf__penalty': (.....), # please explore different regularization terms ('l1', 'l2', 'elasticnet'),\n",
    "                  'clf__l1_ratio': (.15,  .50)}\n",
    "    grid_search = GridSearchCV(.... scoring='precision_macro')                    \n",
    "    #==================================================#\n",
    "    #               Your code ends here                #\n",
    "    #               Please don't add code below here   #\n",
    "    #==================================================#\n",
    "    \n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(data.data, data.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(grid_search.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "         # print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "         print()\n",
    "\n",
    "    '''print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, grid_search.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print() \n",
    "    '''\n",
    "    \n",
    "    scoring='precision_macro'\n",
    "     # Print best accuracy score and best parameter combination\n",
    "    print(\"Best %s score: %0.3f\" %(scoring, grid_search.best_score_))\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    #Sort the grid search results in decreasing order of average         \n",
    "    sortedGridSearchResults = sorted(zip(grid_search.cv_results_[\"params\"], grid_search.cv_results_[\"mean_test_score\"]), \n",
    "           key=lambda x: x[1], reverse=True)\n",
    "    print(f'Top 2 GridSearch results: ({scoring}, hyperparam Combo)\\n {sortedGridSearchResults[0]}\\n {sortedGridSearchResults[1]}\\n\\n\\n')\n",
    "    #print(f'{grid_search.cv_results_['mean_test_score']}')\n",
    "    print(f'{grid_search.cv_results_[\"mean_test_score\"]}')\n",
    "    print(f'{grid_search.cv_results_[\"params\"]}')\n",
    "    print(f'{grid_search.cv_results_}')  #show everything  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix for train data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T15:00:51.217810Z",
     "start_time": "2018-06-27T15:00:43.106400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "preds_train = grid_search.best_estimator_.predict(data.data)\n",
    "\n",
    "cm_train = confusion_matrix(data.target, preds_train).astype(np.float32)\n",
    "cm_train /= cm_train.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "cm_train.size #20 classes by 20 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T15:49:50.237321Z",
     "start_time": "2018-06-27T15:49:42.838377Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(150,150))\n",
    "sns.set(font_scale=3)\n",
    "plt.subplot(121)\n",
    "g = sns.heatmap(cm_train, fmt='.3g', vmin=0, vmax=1, annot=True, cmap=\"Reds\", square=True, cbar_kws={'shrink':.35})\n",
    "plt.xlabel(\"Predicted\", fontsize=54)\n",
    "plt.ylabel(\"True\", fontsize=54)\n",
    "g.set(xticklabels=data.target_names, yticklabels=data.target_names)\n",
    "plt.xticks(rotation='vertical',fontsize=48)\n",
    "plt.yticks(rotation='horizontal',fontsize=48)\n",
    "plt.title(\"\\nConfusion Matrix for 20newsgroups 'TRAIN' Data\\n\", fontsize=54);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix for test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T15:50:08.383098Z",
     "start_time": "2018-06-27T15:50:08.059393Z"
    }
   },
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T15:50:29.507478Z",
     "start_time": "2018-06-27T15:50:24.385054Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_train = grid_search.best_estimator_.predict(data.data)\n",
    "\n",
    "cm_train = confusion_matrix(data.target, preds_train).astype(np.float32)\n",
    "cm_train /= cm_train.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "cm_train.size #20 classes by 20 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T15:50:51.853884Z",
     "start_time": "2018-06-27T15:50:45.048348Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(150,150))\n",
    "sns.set(font_scale=3)\n",
    "plt.subplot(121)\n",
    "g = sns.heatmap(cm_train, fmt='.3g', vmin=0, vmax=1, annot=True, cmap=\"Reds\", square=True, cbar_kws={'shrink':.35})\n",
    "plt.xlabel(\"Predicted\", fontsize=54)\n",
    "plt.ylabel(\"True\", fontsize=54)\n",
    "g.set(xticklabels=data.target_names, yticklabels=data.target_names)\n",
    "plt.xticks(rotation='vertical',fontsize=48)\n",
    "plt.yticks(rotation='horizontal',fontsize=48)\n",
    "plt.title(\"\\nConfusion Matrix for 20newsgroups 'Test' Data\\n\", fontsize=54);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Explore LASSO Logistic Regression\n",
    "Model the CIFAR-10 dataset using LASSO Logistic Regression (l1 penalty term). Explore different values $\\text{C}$ is the inverse regularization constant which is $\\text{C} = \\frac{1}{\\lambda}$ that are listed here : {C=1.0, C=10.0, C=100.0, C=1000.0, C=10000.0}\n",
    "\n",
    "Please reports your experimental results using the `results` table. Add one more column for reporting the number of zero coefficients after training. Recall, LASSO Logistic Regression can be useful in doing feature selection!   \n",
    "\n",
    "NOTE: the coefficient of the learnt model, `model_lr_sklearn`, are available via `model_lr_sklearn.coef_`\n",
    "\n",
    "`coef_ : array, shape (1, n_features) or (n_classes, n_features)\n",
    "\n",
    "Coefficient of the features in the decision function.\n",
    "\n",
    "coef_ is of shape (1, n_features) when the given problem is binary.`\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:  Reload CIFAR-10 dataset and split train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T20:09:13.059634Z",
     "start_time": "2018-06-25T20:09:12.257597Z"
    }
   },
   "outputs": [],
   "source": [
    "for b in range(1, 6):\n",
    "    data_batch = unpickle(\"data/cifar-10-batches-py/data_batch_\" + str(b))\n",
    "    if b == 1:\n",
    "        X_train = data_batch[\"data\"]\n",
    "        y_train = np.array(data_batch[\"labels\"])\n",
    "    else:\n",
    "        X_train = np.append(X_train, data_batch[\"data\"], axis=0)\n",
    "        y_train = np.append(y_train, data_batch[\"labels\"], axis=0)\n",
    "        \n",
    "data_batch = unpickle(\"data/cifar-10-batches-py/test_batch\")\n",
    "X_test = data_batch[\"data\"]\n",
    "y_test = np.array(data_batch[\"labels\"])\n",
    "\n",
    "classes = unpickle(\"data/cifar-10-batches-py/batches.meta\")[\"label_names\"]\n",
    "np.random.seed(42)\n",
    "subsample_rate = 0.02\n",
    "##############################################################################\n",
    "# Set up split for train and test data\n",
    "# Make sure you are using the stratify parameter,\n",
    "# setting the random_state to 42 ans using a subset of 2% of the CIFAR10 dataset\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "X_train, _, y_train, _ = train_test_split()\n",
    "X_test, _, y_test, _ = train_test_split()                 \n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T14:01:19.319900Z",
     "start_time": "2018-06-25T14:01:19.283088Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.float64(X_train)\n",
    "y = np.float64(y_train)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_idx = list(cv.split(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Lasso Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T20:02:53.815231Z",
     "start_time": "2018-06-25T14:01:20.883777Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model_coefs = {}\n",
    "model_acc = {}\n",
    "model_time = {}\n",
    "# Inverse of regularization strength; must be a positive float. \n",
    "# Smaller values specify stronger regularization.\n",
    "c_params = [1.0, 10.0, 100.0, 1000.0, 10000.0] \n",
    "\n",
    "np.random.seed(42)\n",
    "#homegrown grid search \n",
    "for param in c_params:\n",
    "    acc = None\n",
    "    zeros = 0\n",
    "    start_time = time()\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in cv_idx:\n",
    "        print('Calculating the accuracy score for validation fold',fold,'with C =',param)\n",
    "        # split\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        ##############################################################################\n",
    "        # create logistic regression pipeline\n",
    "        # using StandardScaler() and LogisticRegression(..., solver='saga')\n",
    "        # using l1 penalty and the parameters defined above\n",
    "        #==================================================#\n",
    "        #               Your code starts here              #\n",
    "        #==================================================#\n",
    "        pipe =            \n",
    "        #==================================================#\n",
    "        #               Your code ends here                #\n",
    "        #               Please don't add code below here   #\n",
    "        #==================================================#\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        # count zero coefficients\n",
    "        model = pipe.named_steps['lasso']\n",
    "        zeros += np.sum(model.coef_==0)\n",
    "        # generate predictions\n",
    "        y_pred = pipe.predict(X_val)\n",
    "\n",
    "        # evaluate\n",
    "        if acc is None:\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "        else:\n",
    "            acc += accuracy_score(y_val, y_pred)\n",
    "      \n",
    "        fold += 1\n",
    "    # take average of accuracy score and count of zero coefficients for each 'C' value\n",
    "    end_time = time()\n",
    "    model_coefs[param] = round(float(zeros/cv.n_splits),3)\n",
    "    model_acc[param] = round(float(acc/cv.n_splits),3)\n",
    "    model_time[param] = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T20:08:20.871651Z",
     "start_time": "2018-06-25T20:08:20.840793Z"
    }
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"Model Description\", \"Avg Val Accuracy\", \"Average # of Zero Coefficients\", \"Run time\"])\n",
    "for param in c_params: \n",
    "    results.loc[len(results)] = [\"Logistic Regression L1 Reg C=\" + str(int(param)), \n",
    "                                 model_acc[param], model_coefs[param], str(round(model_time[param]/60,2)) + \" mins\"]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK Homegrown implementation of Logistic Regression [OPTIONAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a homegrown implementation of Logistic Regression. In this class  we added an ability to trace validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:49:53.867128Z",
     "start_time": "2018-06-25T21:49:53.473800Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionHomegrown(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for the homgrown Logistic Regression\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "        \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.coef_ = None       # weight vector\n",
    "        self.intercept_ = None  # bias term\n",
    "        self._theta = None      # augmented weight vector, i.e., bias + weights\n",
    "                                # this allows to treat all decision variables homogeneously\n",
    "        self.history = {\"cost\": [], \n",
    "                        \"acc\": [], \n",
    "                        \"val_cost\":[], \n",
    "                        \"val_acc\": []}\n",
    "        \n",
    "    def _grad(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the Logistic Regression \n",
    "        objective function\n",
    "\n",
    "        Args:\n",
    "            X(ndarray):    train objects\n",
    "            y(ndarray):    answers for train objects\n",
    "            \n",
    "        Return:\n",
    "            grad(ndarray): gradient\n",
    "        \"\"\"\n",
    "        # number of training examples\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        # get scores for each class and example\n",
    "        # 2D matrix\n",
    "        scores = self._predict_raw(X)\n",
    "        \n",
    "        # transform scores to probabilities\n",
    "        # softmax\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        # error\n",
    "        probs[range(n),y] -= 1\n",
    "        \n",
    "        # gradient\n",
    "        gradient = np.dot(X.T, probs) / n\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def _gd(self, X, y, max_iter, alpha, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Runs Full GD and logs error, weigths, gradient at every step\n",
    "\n",
    "        Args:\n",
    "            X(ndarray):      train objects\n",
    "            y(ndarray):      answers for train objects\n",
    "            max_iter(int):   number of weight updates\n",
    "            alpha(floar):    step size in direction of gradient\n",
    "            \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for i in range(max_iter):\n",
    "            \n",
    "            metrics = self.score(X, y)\n",
    "            self.history[\"cost\"].append(metrics[\"cost\"])\n",
    "            self.history[\"acc\"].append(metrics[\"acc\"])\n",
    "            \n",
    "            if X_val is not None:\n",
    "                metrics_val = self.score(X_val, y_val)\n",
    "                self.history[\"val_cost\"].append(metrics_val[\"cost\"])\n",
    "                self.history[\"val_acc\"].append(metrics_val[\"acc\"])\n",
    "\n",
    "            # calculate gradient\n",
    "            grad = self._grad(X, y)\n",
    "            \n",
    "            # do gradient step\n",
    "            self._theta -= alpha * grad\n",
    "    \n",
    "    def fit(self, X, y, max_iter=1000, alpha=0.05, val_data=None):\n",
    "        \"\"\"\n",
    "        Public API to fit Logistic regression model\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      train objects\n",
    "            y(ndarray):      answers for train objects\n",
    "            max_iter(int):   number of weight updates\n",
    "            alpha(floar):    step size in direction of gradient\n",
    "            \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Augment the data with the bias term.\n",
    "        # So we can treat the the input variables and the bias term homogeneously \n",
    "        # from a vectorization perspective\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        if val_data is not None:\n",
    "            X_val, y_val = val_data\n",
    "            X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
    "        else:\n",
    "            X_val = None\n",
    "            y_val = None\n",
    "        # initialize if the first step\n",
    "        if self._theta is None:\n",
    "            self._theta = np.random.rand(X.shape[1], len(np.unique(y)))\n",
    "        \n",
    "        # do full gradient descent\n",
    "        self._gd(X, y, max_iter, alpha, X_val, y_val)\n",
    "        \n",
    "        # get final weigths and bias\n",
    "        self.intercept_ = self._theta[0]\n",
    "        self.coef_ = self._theta[1:]\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes logloss and accuracy for (X, y)\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "            y(ndarray):      answers for objects\n",
    "            \n",
    "        Return:\n",
    "            metrics(dict):   python dictionary which\n",
    "                             contains two fields: for accuracy \n",
    "                             and for objective function\n",
    "        \"\"\"\n",
    "        # number of training samples\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        # get scores\n",
    "        scores = self._predict_raw(X)\n",
    "        \n",
    "        # trasnform scores to probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        # logloss per each example\n",
    "        corect_logprobs = -np.log(probs[range(n),y])\n",
    "        \n",
    "        # total mean logloss\n",
    "        data_loss = np.sum(corect_logprobs) / n\n",
    "        \n",
    "        # predictions\n",
    "        pred = np.argmax(scores, axis=1)\n",
    "        # accuracy\n",
    "        acc = accuracy_score(y, pred)\n",
    "        \n",
    "        # final metrics\n",
    "        metrics = {\"acc\": acc, \"cost\": data_loss}\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    def _predict_raw(self, X):\n",
    "        \"\"\"\n",
    "        Computes scores for each class and each object in X\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "        \n",
    "        Return:\n",
    "            scores(ndarray): scores for each class and object\n",
    "        \"\"\"\n",
    "        # check whether X has appended bias feature or not\n",
    "        if X.shape[1] == len(self._theta):\n",
    "            scores = np.dot(X, self._theta)\n",
    "        else:\n",
    "            scores = np.dot(X, self.coef_) + self.intercept_\n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class for each object in X\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "        \n",
    "        Return:\n",
    "            pred(ndarray):   class for each object\n",
    "        \"\"\"\n",
    "        # get scores for each class\n",
    "        scores = self._predict_raw(X)\n",
    "        # choose class with maximum score\n",
    "        pred = np.argmax(scores, axis=1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do not forget** to scale data before using this class. It is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload CIFAR-10 dataset and split train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T20:09:13.059634Z",
     "start_time": "2018-06-25T20:09:12.257597Z"
    }
   },
   "outputs": [],
   "source": [
    "for b in range(1, 6):\n",
    "    data_batch = unpickle(\"data/cifar-10-batches-py/data_batch_\" + str(b))\n",
    "    if b == 1:\n",
    "        X_train = data_batch[\"data\"]\n",
    "        y_train = np.array(data_batch[\"labels\"])\n",
    "    else:\n",
    "        X_train = np.append(X_train, data_batch[\"data\"], axis=0)\n",
    "        y_train = np.append(y_train, data_batch[\"labels\"], axis=0)\n",
    "        \n",
    "data_batch = unpickle(\"data/cifar-10-batches-py/test_batch\")\n",
    "X_test = data_batch[\"data\"]\n",
    "y_test = np.array(data_batch[\"labels\"])\n",
    "\n",
    "classes = unpickle(\"data/cifar-10-batches-py/batches.meta\")[\"label_names\"]\n",
    "np.random.seed(42)\n",
    "subsample_rate = 0.1\n",
    "X_train, _, y_train, _ = train_test_split(X_train, y_train, stratify=y_train, train_size=subsample_rate, random_state=42)\n",
    "X_test, _, y_test, _ = train_test_split(X_test, y_test, stratify=y_test, train_size=subsample_rate, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:50:01.334119Z",
     "start_time": "2018-06-25T21:50:01.184688Z"
    }
   },
   "outputs": [],
   "source": [
    "# scale data \n",
    "np.random.seed(42)\n",
    "if np.max(X_train) > 4.:\n",
    "    X_train = X_train.astype(np.float32) / 255.\n",
    "if np.max(X_test) > 4.:\n",
    "    X_test = X_test.astype(np.float32) / 255.\n",
    "y_train=y_train.astype(int)\n",
    "y_test=y_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:50:10.470600Z",
     "start_time": "2018-06-25T21:50:10.467292Z"
    }
   },
   "outputs": [],
   "source": [
    "model_lr_homegrown = LogisticRegressionHomegrown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:50:13.447439Z",
     "start_time": "2018-06-25T21:50:12.901745Z"
    }
   },
   "outputs": [],
   "source": [
    "model_lr_homegrown.fit(X_train, y_train, max_iter=10, alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we come up with all the $\\text{nan's}$ for objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:50:24.340636Z",
     "start_time": "2018-06-25T21:50:24.333092Z"
    }
   },
   "outputs": [],
   "source": [
    "model_lr_homegrown.history[\"cost\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And accuracy also does not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T20:09:40.009271Z",
     "start_time": "2018-06-25T20:09:40.003557Z"
    }
   },
   "outputs": [],
   "source": [
    "model_lr_homegrown.history[\"acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:51:25.540572Z",
     "start_time": "2018-06-25T21:51:25.513942Z"
    }
   },
   "outputs": [],
   "source": [
    "y_preds = model_lr_homegrown.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:51:56.835089Z",
     "start_time": "2018-06-25T21:51:56.831603Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:51:57.624283Z",
     "start_time": "2018-06-25T21:51:57.619848Z"
    }
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Model','Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:51:58.128469Z",
     "start_time": "2018-06-25T21:51:58.117426Z"
    }
   },
   "outputs": [],
   "source": [
    "results.loc[len(results)] = [\"LR Homegrown\", np.round(acc, 3)]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Softmax Numerical Stability\n",
    "What can be causing this kind of problem (\"NaN\")? (Hint: see the lecture slides \"Numerical Stability for softmax function\")\n",
    "* Please fix this problem\n",
    "* Next section has some background to help shed some light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with np.max and matrices\n",
    "Explore the code in the following section first to see why/how!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:53:11.275714Z",
     "start_time": "2018-06-25T21:53:11.230218Z"
    }
   },
   "outputs": [],
   "source": [
    "## Working with np.max, softmax, and matrices\n",
    "import numpy as np\n",
    "\n",
    "# For more info on np.max check the following link :\n",
    "# https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.amax.html\n",
    "\n",
    "#GOAL: On a per row basis: Substract  max of that row from each element in that row\n",
    "#ATTEMPT #1: \n",
    "f = np.array([[1, 2, 3],[4, 5, 6]])\n",
    "f1 = f - np.max(f)\n",
    "# this does not work as expected - as np.max(f) returns the overall max \n",
    "print (\"Is this correct?\")\n",
    "print (\"Adjusted data1\", f1)\n",
    "\n",
    "# take max of f per row \n",
    "#ATTEMPT #2: pay attention to axis and keepdims\n",
    "f2 = f - np.max(f, axis=1, keepdims=True)\n",
    "print (\"Adjusted data2\", f2)\n",
    "\n",
    "\n",
    "#Normalised score to sum to 1\n",
    "f=([1,3,6], [2,4,6])\n",
    "p = f / np.sum(f,axis=1, keepdims=True)\n",
    "print (\"Normalised score...\\n\", p)\n",
    "\n",
    "# The following data will not work with for the softmax calculation\n",
    "# out of bounds\n",
    "perpendicularDistances = np.array([[123, 234, 345],[444, 555, 999]])\n",
    "probs = np.exp(perpendicularDistances) / np.sum(np.exp(perpendicularDistances), axis=1, keepdims=True)\n",
    "print(\"This produces NaNs: (see SECOND ROW) \\n\", probs)\n",
    "# need to use adjusted data \n",
    "perpendicularDistances -= np.max(perpendicularDistances, axis=1, keepdims=True)\n",
    "# trasnform scores to probabilities\n",
    "exp_scores = np.exp(perpendicularDistances)\n",
    "probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "print (\"Probabilities...\\n\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:53:13.485834Z",
     "start_time": "2018-06-25T21:53:13.465412Z"
    }
   },
   "outputs": [],
   "source": [
    "class FixedLogisticRegressionHomegrown(LogisticRegressionHomegrown):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # call the constructor of the parent class\n",
    "        super(FixedLogisticRegressionHomegrown, self).__init__()\n",
    "        \n",
    "    #==================================================#\n",
    "    #               Place your code here               #\n",
    "    #     Redefine a method which causes the error     #\n",
    "    \n",
    "    #               Hint: only one method              #\n",
    "    #==================================================#\n",
    "    #==================================================#\n",
    "    #               Your code starts here              #\n",
    "    #==================================================#\n",
    "    \n",
    "    def .......(self, X):\n",
    "        \"\"\"\n",
    "        ...... each class and each object in X\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "        \n",
    "        Return:\n",
    "            ...(ndarray): ...... each class and object\n",
    "        \"\"\"\n",
    "        # check whether X has appended bias feature or not\n",
    "        if X.shape[1] == len(self._theta):\n",
    "            scores = np.dot(X, self._theta)\n",
    "        else:\n",
    "            scores = np.dot(X, self.coef_) + self.intercept_\n",
    "        \n",
    "        \n",
    "        \n",
    "        return ...\n",
    "    \n",
    "    #==================================================#\n",
    "    #               Your code ends here                #\n",
    "    #               Please don't add code below here   #\n",
    "    #==================================================#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:53:14.784230Z",
     "start_time": "2018-06-25T21:53:14.780670Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_lr_homegrown_fixed = FixedLogisticRegressionHomegrown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:54:41.877342Z",
     "start_time": "2018-06-25T21:53:15.571240Z"
    }
   },
   "outputs": [],
   "source": [
    "model_lr_homegrown_fixed.fit(X_train, y_train, max_iter=2000, alpha=0.05, val_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:54:42.565836Z",
     "start_time": "2018-06-25T21:54:42.214391Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "plt.suptitle(\"Homegrown Logistic Regression\")\n",
    "plt.subplot(121)\n",
    "plt.plot(model_lr_homegrown_fixed.history[\"cost\"], label=\"Train\")\n",
    "plt.plot(model_lr_homegrown_fixed.history[\"val_cost\"], label=\"Test\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(122)\n",
    "plt.plot(model_lr_homegrown_fixed.history[\"acc\"], label=\"Train\")\n",
    "plt.plot(model_lr_homegrown_fixed.history[\"val_acc\"], label=\"Test\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:54:42.905205Z",
     "start_time": "2018-06-25T21:54:42.886827Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test = model_lr_homegrown_fixed.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:54:43.259784Z",
     "start_time": "2018-06-25T21:54:43.255555Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping table of results up-to-date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:54:43.614928Z",
     "start_time": "2018-06-25T21:54:43.600503Z"
    }
   },
   "outputs": [],
   "source": [
    "results.loc[len(results)] = [\"LR Homegrown Fix 1\", np.round(acc, 3)]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: improve by tuning training algo.\n",
    "Look at the plots obtained at the end of task 1 (it fluctuates a lot). This behaviour is very common to stochastic gradient descent. But here we are using full GD. What could be causing this problem? (*Hint*: a hyperparameter of the training algorithm)\n",
    "* Describe what's going on on the plots that you got\n",
    "* Try to fix it (*Hint*: you do NOT need to change the class implemented before)\n",
    "\n",
    "P.S.\n",
    "Test accuracy before this fix should be about 26%. This should jump to  31% after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:58:55.429379Z",
     "start_time": "2018-06-25T21:54:43.960197Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# Use FixedLogisticRegressionHomegrown() to fit a model\n",
    "# using 6500 for max_iter, a step of 0.02 and \n",
    "# X_test, y_test for validation\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "model_lr_homegrown_fixed = \n",
    "model_lr_homegrown_fixed.fit()             \n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:58:56.234317Z",
     "start_time": "2018-06-25T21:58:55.762778Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "plt.suptitle(\"Homegrown Logistic Regression\")\n",
    "plt.subplot(121)\n",
    "plt.plot(model_lr_homegrown_fixed.history[\"cost\"], label=\"Train\")\n",
    "plt.plot(model_lr_homegrown_fixed.history[\"val_cost\"], label=\"Test\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(122)\n",
    "plt.plot(model_lr_homegrown_fixed.history[\"acc\"], label=\"Train\")\n",
    "plt.plot(model_lr_homegrown_fixed.history[\"val_acc\"], label=\"Test\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:58:56.589537Z",
     "start_time": "2018-06-25T21:58:56.571656Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test = model_lr_homegrown_fixed.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:58:56.951512Z",
     "start_time": "2018-06-25T21:58:56.946931Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T21:58:57.301683Z",
     "start_time": "2018-06-25T21:58:57.289699Z"
    }
   },
   "outputs": [],
   "source": [
    "results.loc[len(results)] = [\"LR Homegrown Fix 2\", np.round(acc, 3)]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note: I was able to get about 29% test set accuracy with all the fixes and tunes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : Visualize the  weight vectors\n",
    "Visualize the  weight vectors (like images) for each class for both Sklearn learnt model and best homegrown learnt model. Can you see any class patterns? How long did it take to run? in both cases?\n",
    "\n",
    "### Task : Use all training data and visualize the  weight vectors\n",
    "Try to use all the training data to learn a classification model  Sklearn and then visual the resulting weight vectors. Notice any differences. How long did it take to run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T20:22:22.582984Z",
     "start_time": "2018-06-25T20:20:27.977511Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_lr_sklearn = LogisticRegression()\n",
    "model_lr_sklearn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T20:22:23.330559Z",
     "start_time": "2018-06-25T20:22:22.902825Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    normalized_coef = (model_lr_sklearn.coef_[i] - np.min(model_lr_sklearn.coef_[i])) \\\n",
    "                      / (np.max(model_lr_sklearn.coef_[i]) - np.min(model_lr_sklearn.coef_[i])) * 255.\n",
    "    show_pic(normalized_coef)\n",
    "    plt.title(classes[i])\n",
    "plt.suptitle(\"Sklearn weight vectors\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T20:19:25.442409Z",
     "start_time": "2018-06-25T20:19:24.992560Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    \n",
    "    # change the code below in order to normalize the coefficients\n",
    "    #==================================================#\n",
    "    #               Your code starts here              #\n",
    "    #==================================================#\n",
    "    # Normalize to [0, 1] interval and then scale to [0, 255]\n",
    "    normalized_coef = (model_lr_sklearn.coef_[i] ........             \n",
    "    #          \n",
    "    #==================================================#\n",
    "    #               Your code ends here                #\n",
    "    #               Please don't add code below here   #\n",
    "    #==================================================#\n",
    "\n",
    "    show_pic(normalized_coef)\n",
    "    plt.title(classes[i])\n",
    "plt.suptitle(\"Homegrown weight vectors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE dimensionality reduction.\n",
    "\n",
    "See [this](https://www.dropbox.com/s/q0lrqtphxh5j4uv/6-Mapping_Representations-tSNE-visualize-DL-Model-Layers.ipynb?dl=0) notebook and also search materials on the internet (e.g. [this](http://distill.pub/2016/misread-tsne/) one) on T-SNE (t-Distributed Stochastic Neighbor Embedding).\n",
    "\n",
    "Perform this embedding (there is [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) implementation) on first three classes and 500 examples per each of them (this is done ro reduce the execution time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T20:23:59.678145Z",
     "start_time": "2018-06-25T20:23:59.621289Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold._t_sne import TSNE\n",
    "tsne = TSNE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T20:24:49.745009Z",
     "start_time": "2018-06-25T20:24:01.184263Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_transformed = tsne.fit_transform(X_train[(y_train == 0) + (y_train == 1) + (y_train == 2)])\n",
    "y_train_transformed = y_train[(y_train == 0) + (y_train == 1) + (y_train == 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T20:24:50.281614Z",
     "start_time": "2018-06-25T20:24:50.070839Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "colors = [\"r\", \"g\", \"b\"]\n",
    "for cl in range(3):\n",
    "    idx = y_train_transformed == cl\n",
    "    plt.scatter(X_train_transformed.T[0][idx], X_train_transformed.T[1][idx], c=colors[cl], label=\"Class \" + str(cl))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "588px",
    "left": "0px",
    "right": "1388px",
    "top": "108px",
    "width": "317.969px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
